{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PGcKvm6aqeWW"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TP_hsEGkqhsz"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoConfig\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from transformers.modeling_outputs import TokenClassifierOutput\n","\n","from tqdm.auto import tqdm\n","\n","from datasets import Dataset, load_metric\n","from datasets import load_metric\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","import re\n","import os\n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":261,"status":"ok","timestamp":1649337110547,"user":{"displayName":"Catarina Barino","userId":"01788286570193106814"},"user_tz":300},"id":"6iwECTWeRN-b"},"outputs":[],"source":["experiment = 'bert_classification_short_vocab'\n","checkpoint = 'bert-base-cased' #\"climatebert/distilroberta-base-climate-s\" # \n","\n","TXT_PATH_TRAIN = '/content/drive/MyDrive/nlp-project/data/sec-filings/txt files 5 key words train/'\n","TXT_PATH_TEST = '/content/drive/MyDrive/nlp-project/data/sec-filings/txt files 5 key words test/'\n","\n","\n","SCORES_PATH = '/content/drive/MyDrive/nlp-project/data/esg-scores/Sustainalytics_scores_original.csv'\n","#SAVE_PATH = '/content/drive/MyDrive/nlp-project/outputs/' + experiment + '.csv'\n","\n","CHECKPOINTS_PATH =   '/content/drive/MyDrive/nlp-project/checkpoints/'\n","WEIGHTS_PATH =   '/content/drive/MyDrive/nlp-project/weights/test-' + experiment + '.pt'\n"," \n","num_epochs = 5\n","num_labels = 5 # 1 for regression, n for n-classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgQtmYjMSOYW"},"outputs":[],"source":["scores = pd.read_csv(SCORES_PATH)\n","scores['Text'] = np.nan\n","scores['Dataset'] = np.nan\n","\n","def clean_text(text):\n","    text = text.replace('\\n', ' ').replace('\\t', '').replace(',', '').replace(';', '')\n","    #regex = '\\xc2\\xb7'\n","    text = re.sub('[^a-zA-Z0-9 \\.]', ' ', text) # remove any characters other than letters, numbers, spaces, periods\n","    text = re.sub(' +', ' ', text) # remove repeated spaces\n","    text = text.strip()\n","\n","    return text\n","\n","for f in os.listdir(TXT_PATH_TRAIN):\n","  try:\n","    row = scores.index[scores['Ticker'] == f.strip('.txt')][0]\n","    \n","    # read text files and remove any newline, tab, comma characters\n","    with open(TXT_PATH_TRAIN + f) as txt_file:\n","      text = txt_file.read()\n","\n","    text = clean_text(text)\n","    scores.loc[row, 'Text'] = text\n","    scores.loc[row, 'Dataset'] = 'train'\n","\n","  except IndexError as e:\n","    print('File ', f, ' not present')\n","\n","for f in os.listdir(TXT_PATH_TEST):\n","  try:\n","    row = scores.index[scores['Ticker'] == f.strip('.txt')][0]\n","    # read text files and remove any newline, tab, comma characters\n","    with open(TXT_PATH_TEST + f) as txt_file:\n","      text = txt_file.read()\n","\n","    text = clean_text(text)\n","    scores.loc[row, 'Text'] = text\n","    scores.loc[row, 'Dataset'] = 'test'\n","  except IndexError as e:\n","    print('File ', f, ' not present')\n","    \n","scores = scores.dropna()\n","scores = scores.reset_index(drop=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2235,"status":"ok","timestamp":1649337043067,"user":{"displayName":"Catarina Barino","userId":"01788286570193106814"},"user_tz":300},"id":"rC1Bru7XATH7"},"outputs":[],"source":["# split the data for long text \n","def longtext_split(text):\n","  total = []\n","\n","  if len(text.split()) // 150 > 0:\n","    n = len(text.split()) // 150\n","  else:\n","    n = 1\n","\n","  for w in range(n):\n","    if w == 0:\n","      partial = text.split()[:200]\n","    else:\n","      partial = text.split()[w*150:w*150+200]\n","    total.append(' '.join(partial))\n","\n","  return total\n","\n","scores['text_split'] = scores['Text'].apply(longtext_split)\n","scores = scores.explode('text_split')\n","      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viUzEMjMiwsD"},"outputs":[],"source":["scores.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfRtruRG37vB"},"outputs":[],"source":["#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"single_label_classification\", num_labels=num_labels)\n","\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model =  AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"]},{"cell_type":"code","source":["train_df = scores.loc[scores['Dataset'] == 'train']\n","test_df = scores.loc[scores['Dataset'] == 'test']\n","del scores"],"metadata":{"id":"kHPrpQbV-eLh","executionInfo":{"status":"ok","timestamp":1649337054122,"user_tz":300,"elapsed":37,"user":{"displayName":"Catarina Barino","userId":"01788286570193106814"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6-YMJJxqVA_"},"outputs":[],"source":["def transform_labels(label):\n","  label = label['Sustainalytics Class']\n","  \n","  if label == 'Negligible':\n","    num = 0\n","  elif label == 'Low':\n","    num = 1\n","  elif label == 'Medium':\n","    num = 2\n","  elif label == 'High':\n","    num = 3\n","  elif label == 'Severe':\n","    num = 4\n","\n","  return {'labels': num}\n","\n","\n","def encode(example):\n","    return tokenizer(example['text_split'], example['Industry'], truncation=True, padding='max_length') \n","\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","# encode data\n","train_dataset = train_dataset.map(encode, batched=True) \n","test_dataset = test_dataset.map(encode, batched=True)\n","\n","# transform labels to integers\n","remove_columns = ['Sustainalytics Score', 'Company Name', 'Ticker', 'Sustainalytics Class', 'Text', '__index_level_0__', 'text_split', 'Industry', 'Dataset']\n","train_dataset = train_dataset.map(transform_labels, remove_columns=remove_columns) # change target column name to labels\n","test_dataset = test_dataset.map(transform_labels, remove_columns=remove_columns) # change target column name to labels\n","\n","train_dataset = train_dataset.shuffle(seed=10)\n","test_dataset = test_dataset.shuffle(seed=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aP71E9HqNeFX"},"outputs":[],"source":["training_args = TrainingArguments(output_dir=CHECKPOINTS_PATH, num_train_epochs=num_epochs)\n","trainer = Trainer(model = model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\n","trainer.train()\n","torch.save(model.state_dict(), WEIGHTS_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwbVGgrgqVy6"},"outputs":[],"source":["outputs = trainer.predict(test_dataset)"]},{"cell_type":"code","source":["np.argmax(outputs.predictions, axis=-1)"],"metadata":{"id":"5J6_VxwXayoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJAwPXLvx41J"},"outputs":[],"source":["metric = load_metric('accuracy')\n","def compute_metrics(eval_pred):\n","  logits, labels, _ = eval_pred\n","  predictions = np.argmax(logits, axis=-1)\n","\n","  return metric.compute(predictions=predictions, references=labels)\n","\n","print(compute_metrics(outputs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iOk_9nyytCM"},"outputs":[],"source":["metric = load_metric('f1')\n","def compute_metrics(eval_pred):\n","  logits, labels, _ = eval_pred\n","  predictions = np.argmax(logits, axis=-1)\n","\n","  return metric.compute(predictions=predictions, references=labels, average=None)\n","\n","print(compute_metrics(outputs))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"bert-esg-training.ipynb","provenance":[],"authorship_tag":"ABX9TyMz00qY8U5tcWsMSMIcWDus"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}